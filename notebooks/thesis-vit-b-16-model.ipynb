{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13099300,"sourceType":"datasetVersion","datasetId":8297594},{"sourceId":13132068,"sourceType":"datasetVersion","datasetId":8319197},{"sourceId":13147202,"sourceType":"datasetVersion","datasetId":8329708},{"sourceId":13147408,"sourceType":"datasetVersion","datasetId":8329859}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport timm\nimport cv2\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, f1_score\nfrom collections import defaultdict\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üî• Using device: {device}\")\n\n# ==================== DATASET CLASS ====================\n\nclass CloveGradingDataset(Dataset):\n    \"\"\"\n    Dataset for clove grading - optimized for ViT\n    \"\"\"\n    def __init__(self, data_dir, transform=None, resolution='224x224'):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.resolution = resolution\n        self.samples = []\n        \n        # Collect all samples from grade folders\n        grade_mapping = {'Grade 1': 0, 'Grade 2': 1, 'Grade 3': 2, 'Grade 4': 3,\n                        'Grade_1': 0, 'Grade_2': 1, 'Grade_3': 2, 'Grade_4': 3}\n        \n        for grade_folder in os.listdir(data_dir):\n            grade_path = os.path.join(data_dir, grade_folder)\n            if not os.path.isdir(grade_path):\n                continue\n            \n            # Get label\n            label = grade_mapping.get(grade_folder)\n            if label is None:\n                continue\n            \n            # Collect images\n            for img_file in os.listdir(grade_path):\n                if img_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n                    img_path = os.path.join(grade_path, img_file)\n                    self.samples.append((img_path, label))\n        \n        print(f\"üìä Loaded {len(self.samples)} samples\")\n        \n        # Print class distribution\n        labels = [s[1] for s in self.samples]\n        for i in range(4):\n            count = labels.count(i)\n            print(f\"   Grade {i+1}: {count} samples ({count/len(labels)*100:.1f}%)\")\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        \n        # Load image\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Apply transform\n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = transforms.ToTensor()(image)\n        \n        return image, label\n\n# ==================== VISION TRANSFORMER (VIT) MODEL ====================\n\nclass VisionTransformer:\n    \"\"\"Vision Transformer (ViT-B/16) implementation\"\"\"\n    \n    @staticmethod\n    def create_model(model_name='vit_base_patch16_224', num_classes=4, pretrained=True, img_size=224):\n        \"\"\"\n        Create Vision Transformer model\n        \n        Args:\n            model_name: 'vit_base_patch16_224' (ViT-B/16)\n            num_classes: Number of output classes\n            pretrained: Use pretrained weights\n            img_size: Input image size (224 for ViT-B/16)\n        \n        Returns:\n            model: PyTorch model\n        \"\"\"\n        \n        # Available ViT models\n        vit_models = [\n            'vit_base_patch16_224',\n            'vit_base_patch16_384',\n            'vit_large_patch16_224',\n            'vit_large_patch16_384'\n        ]\n        \n        if model_name not in vit_models:\n            raise ValueError(f\"Model {model_name} not supported. Available: {vit_models}\")\n        \n        print(f\"üîß Creating {model_name}...\")\n        print(f\"   Input size: {img_size}x{img_size}\")\n        print(f\"   Patch size: 16x16\")\n        print(f\"   Embedding dimension: 768\")\n        \n        # Create ViT model\n        model = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=num_classes,\n            img_size=img_size\n        )\n        \n        # Enhanced head for better classification\n        if hasattr(model, 'head'):\n            # Original head structure\n            in_features = model.head.in_features\n            \n            # Replace with enhanced classification head\n            model.head = nn.Sequential(\n                nn.LayerNorm(in_features),\n                nn.Dropout(0.5),\n                nn.Linear(in_features, 1024),\n                nn.GELU(),\n                nn.Dropout(0.3),\n                nn.Linear(1024, 512),\n                nn.GELU(),\n                nn.Dropout(0.2),\n                nn.Linear(512, num_classes)\n            )\n        \n        return model\n    \n    @staticmethod\n    def get_model_info(model_name='vit_base_patch16_224'):\n        \"\"\"Get ViT model parameters and characteristics\"\"\"\n        model_info = {\n            'vit_base_patch16_224': {\n                'params': '86.6M',\n                'input_size': 224,\n                'patch_size': 16,\n                'embed_dim': 768,\n                'depth': 12,\n                'heads': 12,\n                'mlp_ratio': 4.0,\n                'speed': 'Medium',\n                'description': 'Base Vision Transformer with 16x16 patches',\n                'strengths': 'Global attention, excellent for structured patterns',\n                'attention_type': 'Global self-attention',\n                'memory': 'High'\n            },\n            'vit_base_patch16_384': {\n                'params': '86.6M',\n                'input_size': 384,\n                'patch_size': 16,\n                'embed_dim': 768,\n                'depth': 12,\n                'heads': 12,\n                'mlp_ratio': 4.0,\n                'speed': 'Slow',\n                'description': 'Base ViT with higher resolution',\n                'strengths': 'Better fine-grained details',\n                'attention_type': 'Global self-attention',\n                'memory': 'Very High'\n            }\n        }\n        return model_info.get(model_name, {'params': 'Unknown', 'speed': 'Unknown'})\n    \n    @staticmethod\n    def visualize_attention(model, image_tensor, save_path=None):\n        \"\"\"\n        Visualize attention maps from ViT\n        \"\"\"\n        model.eval()\n        \n        # Register hook to get attention weights\n        attention_weights = []\n        \n        def hook_fn(module, input, output):\n            attention_weights.append(output[1])  # Attention weights\n        \n        # Register hooks on all attention blocks\n        for block in model.blocks:\n            block.attn.register_forward_hook(hook_fn)\n        \n        # Forward pass\n        with torch.no_grad():\n            _ = model(image_tensor.unsqueeze(0).to(device))\n        \n        # Process attention weights\n        if attention_weights:\n            # Get attention from last layer\n            attention = attention_weights[-1].cpu().numpy()\n            attention = attention[0]  # Remove batch dimension\n            \n            # Average over heads\n            attention = attention.mean(axis=0)\n            \n            # Get CLS token attention to patches\n            cls_attention = attention[0, 1:]  # Skip CLS token itself\n            \n            # Reshape to patch grid\n            grid_size = int(np.sqrt(cls_attention.shape[0]))\n            attention_map = cls_attention.reshape(grid_size, grid_size)\n            \n            # Upsample to image size\n            attention_map = torch.from_numpy(attention_map).unsqueeze(0).unsqueeze(0)\n            attention_map = nn.functional.interpolate(\n                attention_map, \n                size=(image_tensor.shape[1], image_tensor.shape[2]), \n                mode='bilinear', \n                align_corners=False\n            ).squeeze().numpy()\n            \n            # Normalize\n            attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n            \n            # Visualize\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n            \n            # Original image\n            img = image_tensor.permute(1, 2, 0).cpu().numpy()\n            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n            img = np.clip(img, 0, 1)\n            \n            ax1.imshow(img)\n            ax1.set_title('Original Image')\n            ax1.axis('off')\n            \n            # Attention heatmap\n            im = ax2.imshow(attention_map, cmap='jet')\n            ax2.set_title('ViT Attention Map (CLS token to patches)')\n            ax2.axis('off')\n            plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n            \n            if save_path:\n                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            \n            plt.tight_layout()\n            plt.show()\n            \n            return attention_map\n        \n        return None\n\n# ==================== VIT-SPECIFIC TRAINING FRAMEWORK ====================\n\nclass ViTTrainer:\n    \"\"\"Specialized trainer for Vision Transformer models\"\"\"\n    \n    def __init__(self, model, device, train_loader, val_loader, test_loader, model_name):\n        self.model = model.to(device)\n        self.device = device\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.test_loader = test_loader\n        self.model_name = model_name\n        \n        # ViT-specific hyperparameters\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n        \n        # AdamW optimizer with ViT-specific settings\n        self.optimizer = optim.AdamW(\n            model.parameters(),\n            lr=3e-4,  # Higher LR for transformers\n            betas=(0.9, 0.999),\n            weight_decay=0.05,  # Higher weight decay\n            eps=1e-8\n        )\n        \n        # Cosine annealing with warmup (critical for ViT)\n        self.warmup_epochs = 5\n        self.total_epochs = 50\n        \n        # Create custom scheduler\n        self.scheduler = self._create_scheduler()\n        \n        # Gradient scaling for stability\n        self.scaler = torch.cuda.amp.GradScaler()\n        \n        # Training history\n        self.history = {\n            'train_loss': [], 'train_acc': [], 'train_f1': [],\n            'val_loss': [], 'val_acc': [], 'val_f1': [],\n            'test_acc': None, 'test_f1': None,\n            'learning_rates': [],\n            'gradient_norms': [],\n            'attention_maps': []\n        }\n    \n    def _create_scheduler(self):\n        \"\"\"Create learning rate scheduler with warmup\"\"\"\n        # Linear warmup followed by cosine decay\n        def lr_lambda(epoch):\n            if epoch < self.warmup_epochs:\n                # Linear warmup\n                return (epoch + 1) / self.warmup_epochs\n            else:\n                # Cosine decay\n                progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n                return 0.5 * (1 + np.cos(np.pi * progress))\n        \n        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n    \n    def train_epoch(self, epoch):\n        \"\"\"Train for one epoch with ViT-specific optimizations\"\"\"\n        self.model.train()\n        total_loss = 0\n        all_preds = []\n        all_labels = []\n        gradient_norms = []\n        \n        for batch_idx, (images, labels) in enumerate(tqdm(self.train_loader, desc=f\"Training {self.model_name}\")):\n            images, labels = images.to(self.device), labels.to(self.device)\n            \n            self.optimizer.zero_grad()\n            \n            # Mixed precision training\n            with torch.cuda.amp.autocast():\n                outputs = self.model(images)\n                loss = self.criterion(outputs, labels)\n            \n            # Scale loss and backward\n            self.scaler.scale(loss).backward()\n            \n            # Unscale for gradient clipping\n            self.scaler.unscale_(self.optimizer)\n            \n            # Gradient clipping (important for transformers)\n            grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n            gradient_norms.append(grad_norm.item())\n            \n            # Step optimizer\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            \n            total_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n        \n        # Step scheduler\n        self.scheduler.step()\n        \n        avg_loss = total_loss / len(self.train_loader)\n        accuracy = accuracy_score(all_labels, all_preds)\n        f1 = f1_score(all_labels, all_preds, average='weighted')\n        avg_grad_norm = np.mean(gradient_norms)\n        \n        return avg_loss, accuracy, f1, avg_grad_norm\n    \n    def validate(self, loader):\n        \"\"\"Validate on given loader\"\"\"\n        self.model.eval()\n        total_loss = 0\n        all_preds = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for images, labels in loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                \n                with torch.cuda.amp.autocast():\n                    outputs = self.model(images)\n                    loss = self.criterion(outputs, labels)\n                \n                total_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n        \n        avg_loss = total_loss / len(loader)\n        accuracy = accuracy_score(all_labels, all_preds)\n        f1 = f1_score(all_labels, all_preds, average='weighted')\n        \n        return avg_loss, accuracy, f1, all_preds, all_labels\n    \n    def train(self, num_epochs=50, save_path=None):\n        \"\"\"Complete training loop for ViT\"\"\"\n        print(f\"\\nüöÄ Training {self.model_name} - Vision Transformer\")\n        print(\"=\" * 80)\n        \n        # Print model info\n        model_info = VisionTransformer.get_model_info(self.model_name)\n        print(f\"üìä Model: {self.model_name}\")\n        print(f\"   Parameters: {model_info['params']}\")\n        print(f\"   Input Size: {model_info['input_size']}x{model_info['input_size']}\")\n        print(f\"   Patch Size: {model_info['patch_size']}x{model_info['patch_size']}\")\n        print(f\"   Depth: {model_info['depth']} transformer blocks\")\n        print(f\"   Attention Heads: {model_info['heads']}\")\n        print(f\"   Description: {model_info['description']}\")\n        print(f\"   Strengths: {model_info['strengths']}\")\n        \n        print(f\"\\n‚ö° Training Configuration:\")\n        print(f\"   Warmup epochs: {self.warmup_epochs}\")\n        print(f\"   Total epochs: {num_epochs}\")\n        print(f\"   Learning rate: {self.optimizer.param_groups[0]['lr']}\")\n        print(f\"   Weight decay: {self.optimizer.param_groups[0]['weight_decay']}\")\n        \n        best_val_f1 = 0\n        patience_counter = 0\n        max_patience = 15\n        \n        for epoch in range(num_epochs):\n            # Train\n            train_loss, train_acc, train_f1, grad_norm = self.train_epoch(epoch)\n            self.history['train_loss'].append(train_loss)\n            self.history['train_acc'].append(train_acc)\n            self.history['train_f1'].append(train_f1)\n            self.history['gradient_norms'].append(grad_norm)\n            \n            # Validate\n            val_loss, val_acc, val_f1, _, _ = self.validate(self.val_loader)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n            self.history['val_f1'].append(val_f1)\n            self.history['learning_rates'].append(self.optimizer.param_groups[0]['lr'])\n            \n            # Check if in warmup phase\n            phase = \"WARMUP\" if epoch < self.warmup_epochs else \"TRAINING\"\n            \n            print(f\"\\nüìà Epoch {epoch+1}/{num_epochs} [{phase}]:\")\n            print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n            print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n            print(f\"  Grad Norm: {grad_norm:.2f}, LR: {self.optimizer.param_groups[0]['lr']:.2e}\")\n            \n            # Save best model\n            if val_f1 > best_val_f1:\n                best_val_f1 = val_f1\n                patience_counter = 0\n                if save_path:\n                    torch.save({\n                        'model_state_dict': self.model.state_dict(),\n                        'optimizer_state_dict': self.optimizer.state_dict(),\n                        'scheduler_state_dict': self.scheduler.state_dict(),\n                        'scaler_state_dict': self.scaler.state_dict(),\n                        'epoch': epoch,\n                        'val_f1': val_f1,\n                        'history': self.history\n                    }, save_path)\n                    print(f\"  üíæ New best model saved! F1: {val_f1:.4f}\")\n            else:\n                patience_counter += 1\n            \n            if patience_counter >= max_patience:\n                print(f\"‚èπÔ∏è Early stopping triggered\")\n                break\n            \n            # Visualize attention for first epoch and best epoch\n            if epoch == 0 or epoch == num_epochs - 1 or val_f1 == best_val_f1:\n                if len(self.train_loader) > 0:\n                    # Get a sample image\n                    sample_image, _ = next(iter(self.train_loader))\n                    if len(sample_image) > 0:\n                        attention_map = VisionTransformer.visualize_attention(\n                            self.model, \n                            sample_image[0].cpu(),\n                            save_path=f'/kaggle/working/attention_epoch_{epoch+1}.png' if save_path else None\n                        )\n                        if attention_map is not None:\n                            self.history['attention_maps'].append({\n                                'epoch': epoch,\n                                'val_f1': val_f1,\n                                'attention_map': attention_map.tolist()  # Convert to list for JSON serialization\n                            })\n        \n        # Final test evaluation\n        print(f\"\\nüìä Final Test Evaluation:\")\n        test_loss, test_acc, test_f1, test_preds, test_labels = self.validate(self.test_loader)\n        self.history['test_acc'] = test_acc\n        self.history['test_f1'] = test_f1\n        \n        print(f\"Test - Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, F1: {test_f1:.4f}\")\n        \n        # Detailed analysis\n        self._detailed_analysis(test_preds, test_labels)\n        \n        # Plot training history\n        self.plot_training_history()\n        \n        # Visualize final attention\n        if len(self.train_loader) > 0:\n            sample_image, _ = next(iter(self.train_loader))\n            if len(sample_image) > 0:\n                VisionTransformer.visualize_attention(\n                    self.model, \n                    sample_image[0].cpu(),\n                    save_path='/kaggle/working/attention_final.png'\n                )\n        \n        return self.history\n    \n    def _detailed_analysis(self, preds, labels):\n        \"\"\"Perform detailed analysis of results\"\"\"\n        # Classification report\n        print(\"\\nüìã Detailed Classification Report:\")\n        print(classification_report(labels, preds, target_names=['Grade 1', 'Grade 2', 'Grade 3', 'Grade 4']))\n        \n        # Confusion matrix\n        cm = confusion_matrix(labels, preds)\n        self.plot_confusion_matrix(cm)\n        \n        # Per-class metrics\n        print(\"\\nüéØ Per-Class Performance:\")\n        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n        for i, class_name in enumerate(['Grade 1', 'Grade 2', 'Grade 3', 'Grade 4']):\n            print(f\"   {class_name}: Precision={precision[i]:.3f}, Recall={recall[i]:.3f}, F1={f1[i]:.3f}\")\n        \n        # Transformer-specific metrics\n        print(f\"\\nüîç Transformer Insights:\")\n        print(f\"   Number of misclassified samples: {np.sum(np.array(preds) != np.array(labels))}\")\n        print(f\"   Most confused pair: {self._get_most_confused_pair(preds, labels)}\")\n    \n    def _get_most_confused_pair(self, preds, labels):\n        \"\"\"Find most confused class pair\"\"\"\n        cm = confusion_matrix(labels, preds)\n        np.fill_diagonal(cm, 0)  # Ignore correct predictions\n        \n        if cm.sum() > 0:\n            max_idx = np.unravel_index(cm.argmax(), cm.shape)\n            return f\"Class {max_idx[0]+1} ‚Üí Class {max_idx[1]+1} ({cm[max_idx]} samples)\"\n        return \"No misclassifications\"\n    \n    def plot_confusion_matrix(self, cm):\n        \"\"\"Plot confusion matrix\"\"\"\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                   xticklabels=['Grade 1', 'Grade 2', 'Grade 3', 'Grade 4'],\n                   yticklabels=['Grade 1', 'Grade 2', 'Grade 3', 'Grade 4'])\n        plt.title(f'Confusion Matrix - {self.model_name}')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.tight_layout()\n        plt.savefig(f'/kaggle/working/confusion_matrix_{self.model_name}.png', \n                   dpi=300, bbox_inches='tight')\n        plt.show()\n    \n    def plot_training_history(self):\n        \"\"\"Plot comprehensive training history\"\"\"\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n        \n        epochs = range(1, len(self.history['train_loss']) + 1)\n        \n        # Loss\n        ax1.plot(epochs, self.history['train_loss'], 'b-', label='Train Loss', linewidth=2, alpha=0.8)\n        ax1.plot(epochs, self.history['val_loss'], 'r-', label='Val Loss', linewidth=2, alpha=0.8)\n        ax1.axvline(self.warmup_epochs, color='gray', linestyle='--', alpha=0.5, label='Warmup End')\n        ax1.set_title(f'ViT Training History - Loss', fontsize=14, fontweight='bold')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Accuracy\n        ax2.plot(epochs, self.history['train_acc'], 'b-', label='Train Accuracy', linewidth=2, alpha=0.8)\n        ax2.plot(epochs, self.history['val_acc'], 'r-', label='Val Accuracy', linewidth=2, alpha=0.8)\n        ax2.axvline(self.warmup_epochs, color='gray', linestyle='--', alpha=0.5, label='Warmup End')\n        ax2.set_title(f'ViT Training History - Accuracy', fontsize=14, fontweight='bold')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Accuracy')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        # F1 Score\n        ax3.plot(epochs, self.history['train_f1'], 'b-', label='Train F1', linewidth=2, alpha=0.8)\n        ax3.plot(epochs, self.history['val_f1'], 'r-', label='Val F1', linewidth=2, alpha=0.8)\n        ax3.axvline(self.warmup_epochs, color='gray', linestyle='--', alpha=0.5, label='Warmup End')\n        ax3.set_title(f'ViT Training History - F1 Score', fontsize=14, fontweight='bold')\n        ax3.set_xlabel('Epoch')\n        ax3.set_ylabel('F1 Score')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n        \n        # Learning rate schedule\n        ax4.plot(epochs, self.history['learning_rates'], 'g-', linewidth=2, alpha=0.8)\n        ax4.axvline(self.warmup_epochs, color='gray', linestyle='--', alpha=0.5, label='Warmup End')\n        ax4.set_xlabel('Epoch')\n        ax4.set_ylabel('Learning Rate')\n        ax4.set_title(f'ViT Learning Rate Schedule', fontsize=14, fontweight='bold')\n        ax4.set_yscale('log')\n        ax4.grid(True, alpha=0.3)\n        \n        # Add warmup annotation\n        ax4.annotate('Warmup Phase', \n                    xy=(self.warmup_epochs/2, self.history['learning_rates'][self.warmup_epochs//2]),\n                    xytext=(self.warmup_epochs/2, self.history['learning_rates'][self.warmup_epochs//2] * 5),\n                    arrowprops=dict(arrowstyle='->', color='black'),\n                    fontweight='bold')\n        \n        plt.tight_layout()\n        plt.savefig(f'/kaggle/working/vit_training_history_{self.model_name}.png', \n                   dpi=300, bbox_inches='tight')\n        plt.show()\n\n# ==================== VIT EXPERIMENT MANAGER ====================\n\nclass ViTExperimentManager:\n    \"\"\"Manage Vision Transformer experiments\"\"\"\n    \n    def __init__(self, data_dir, resolution='224x224', batch_size=16):\n        self.data_dir = data_dir\n        self.resolution = resolution\n        self.batch_size = batch_size\n        self.results = []\n    \n    def prepare_dataloaders(self):\n        \"\"\"Prepare train/val/test dataloaders for ViT\"\"\"\n        \n        # ViT requires specific preprocessing\n        img_size = 224  # ViT-B/16 standard size\n        \n        # ViT-optimized transforms\n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((img_size, img_size)),\n            transforms.RandomHorizontalFlip(0.5),\n            transforms.RandomRotation(15),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # ViT normalization\n        ])\n        \n        val_transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n        \n        # Create full dataset\n        full_dataset = CloveGradingDataset(\n            self.data_dir, \n            transform=None,\n            resolution=self.resolution\n        )\n        \n        # Split dataset\n        train_size = int(0.7 * len(full_dataset))\n        val_size = int(0.15 * len(full_dataset))\n        test_size = len(full_dataset) - train_size - val_size\n        \n        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n            full_dataset, [train_size, val_size, test_size],\n            generator=torch.Generator().manual_seed(42)\n        )\n        \n        # Apply transforms\n        train_dataset.dataset.transform = transform\n        val_dataset.dataset.transform = val_transform\n        test_dataset.dataset.transform = val_transform\n        \n        # Create dataloaders\n        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n        \n        print(f\"‚úÖ ViT Data Preparation Complete:\")\n        print(f\"   Input size: {img_size}x{img_size}\")\n        print(f\"   Batch size: {self.batch_size}\")\n        print(f\"   Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n        print(f\"   Normalization: mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]\")\n        \n        return train_loader, val_loader, test_loader\n    \n    def run_vit_experiment(self, model_name='vit_base_patch16_224'):\n        \"\"\"\n        Run Vision Transformer experiment\n        \n        Args:\n            model_name: ViT model name\n        \"\"\"\n        print(f\"\\n{'='*80}\")\n        print(f\"üöÄ VISION TRANSFORMER EXPERIMENT: {model_name.upper()}\")\n        print(f\"{'='*80}\")\n        \n        # Prepare data\n        train_loader, val_loader, test_loader = self.prepare_dataloaders()\n        \n        # Create model\n        model = VisionTransformer.create_model(\n            model_name=model_name,\n            num_classes=4,\n            pretrained=True,\n            img_size=224\n        )\n        \n        # Train model\n        trainer = ViTTrainer(\n            model, device, train_loader, val_loader, test_loader,\n            model_name=model_name\n        )\n        \n        save_path = f\"/kaggle/working/best_{model_name}.pth\"\n        history = trainer.train(num_epochs=50, save_path=save_path)\n        \n        # Store results\n        result = {\n            'model_name': model_name,\n            'resolution': self.resolution,\n            'test_accuracy': history['test_acc'],\n            'test_f1': history['test_f1'],\n            'best_val_f1': max(history['val_f1']),\n            'final_train_acc': history['train_acc'][-1],\n            'final_val_acc': history['val_acc'][-1],\n            'train_val_gap': history['train_acc'][-1] - history['val_acc'][-1],\n            'model_info': VisionTransformer.get_model_info(model_name),\n            'attention_maps': history.get('attention_maps', []),\n            'history': history\n        }\n        \n        self.results.append(result)\n        \n        return result\n    \n    def compare_results(self):\n        \"\"\"Display ViT results\"\"\"\n        if not self.results:\n            print(\"No results to compare!\")\n            return\n        \n        # Create results DataFrame\n        df_data = []\n        for r in self.results:\n            df_data.append({\n                'Model': r['model_name'],\n                'Parameters': r['model_info']['params'],\n                'Input Size': f\"{r['model_info']['input_size']}x{r['model_info']['input_size']}\",\n                'Test Accuracy': f\"{r['test_accuracy']:.4f}\",\n                'Test F1': f\"{r['test_f1']:.4f}\",\n                'Best Val F1': f\"{r['best_val_f1']:.4f}\",\n                'Train-Val Gap': f\"{r['train_val_gap']:.4f}\",\n                'Attention Heads': r['model_info']['heads'],\n                'Transformer Depth': r['model_info']['depth']\n            })\n        \n        df = pd.DataFrame(df_data)\n        \n        print(\"\\nüìä VISION TRANSFORMER RESULTS\")\n        print(\"=\" * 100)\n        print(df.to_string(index=False))\n        \n        # Save to CSV\n        df.to_csv('/kaggle/working/vit_results.csv', index=False)\n        print(\"\\n‚úÖ Results saved to 'vit_results.csv'\")\n        \n        # Plot results\n        self.plot_results()\n        \n        # Save detailed results\n        with open('/kaggle/working/vit_detailed_results.json', 'w') as f:\n            # Convert numpy arrays to lists for JSON serialization\n            import copy\n            results_copy = copy.deepcopy(self.results)\n            for r in results_copy:\n                if 'history' in r:\n                    # Remove large arrays if needed\n                    pass\n            json.dump(results_copy, f, indent=2, default=str)\n    \n    def plot_results(self):\n        \"\"\"Plot ViT results\"\"\"\n        if not self.results:\n            return\n        \n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n        \n        model_name = self.results[0]['model_name']\n        history = self.results[0]['history']\n        \n        epochs = range(1, len(history['train_loss']) + 1)\n        \n        # Training curves\n        ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n        ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss')\n        ax1.set_title(f'{model_name} - Training & Validation Loss', fontsize=14, fontweight='bold')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        ax2.plot(epochs, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n        ax2.plot(epochs, history['val_acc'], 'r-', label='Val Accuracy', linewidth=2)\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Accuracy')\n        ax2.set_title(f'{model_name} - Training & Validation Accuracy', fontsize=14, fontweight='bold')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        # Learning rate\n        ax3.plot(epochs, history['learning_rates'], 'g-', linewidth=2)\n        ax3.set_xlabel('Epoch')\n        ax3.set_ylabel('Learning Rate')\n        ax3.set_title(f'{model_name} - Learning Rate Schedule', fontsize=14, fontweight='bold')\n        ax3.set_yscale('log')\n        ax3.grid(True, alpha=0.3)\n        \n        # Model architecture info\n        ax4.axis('off')\n        model_info = self.results[0]['model_info']\n        info_text = f\"\"\"\n        üèóÔ∏è Model Architecture:\n        \n        ‚Ä¢ Model: {model_name}\n        ‚Ä¢ Parameters: {model_info['params']}\n        ‚Ä¢ Input Size: {model_info['input_size']}x{model_info['input_size']}\n        ‚Ä¢ Patch Size: {model_info['patch_size']}x{model_info['patch_size']}\n        ‚Ä¢ Embedding Dim: {model_info['embed_dim']}\n        ‚Ä¢ Depth: {model_info['depth']} blocks\n        ‚Ä¢ Attention Heads: {model_info['heads']}\n        ‚Ä¢ MLP Ratio: {model_info['mlp_ratio']}\n        \n        üéØ Final Results:\n        \n        ‚Ä¢ Test Accuracy: {self.results[0]['test_accuracy']:.4f}\n        ‚Ä¢ Test F1: {self.results[0]['test_f1']:.4f}\n        ‚Ä¢ Best Val F1: {self.results[0]['best_val_f1']:.4f}\n        \"\"\"\n        \n        ax4.text(0.1, 0.9, info_text, transform=ax4.transAxes, fontsize=12,\n                verticalalignment='top', bbox=dict(boxstyle=\"round,pad=1.0\", facecolor=\"lightblue\"))\n        \n        plt.tight_layout()\n        plt.savefig(f'/kaggle/working/vit_results_summary_{model_name}.png', \n                   dpi=300, bbox_inches='tight')\n        plt.show()\n\n# ==================== MAIN EXECUTION ====================\n\ndef run_vit_analysis():\n    \"\"\"\n    MAIN FUNCTION: Run Vision Transformer analysis\n    \"\"\"\n    print(\"\\nüöÄ VISION TRANSFORMER (ViT-B/16) ANALYSIS\")\n    print(\"=\" * 80)\n    \n    # Show system info\n    print(f\"System: {device}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name()}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    \n    # Initialize manager\n    manager = ViTExperimentManager(\n        data_dir=\"/kaggle/input/processed-images-224x224\",\n        resolution='224x224',\n        batch_size=16\n    )\n    \n    # ViT model to test\n    vit_model = 'vit_base_patch16_224'\n    \n    print(f\"\\nüéØ TESTING VISION TRANSFORMER:\")\n    model_info = VisionTransformer.get_model_info(vit_model)\n    print(f\"   Model: {vit_model}\")\n    print(f\"   Parameters: {model_info['params']}\")\n    print(f\"   Architecture: {model_info['description']}\")\n    print(f\"   Strengths: {model_info['strengths']}\")\n    \n    print(f\"\\n‚ö° ViT-SPECIFIC FEATURES:\")\n    print(f\"   ‚Ä¢ Global self-attention mechanism\")\n    print(f\"   ‚Ä¢ 16x16 patch embedding\")\n    print(f\"   ‚Ä¢ {model_info['depth']} transformer blocks\")\n    print(f\"   ‚Ä¢ {model_info['heads']} attention heads\")\n    print(f\"   ‚Ä¢ Cosine learning rate schedule with warmup\")\n    \n    print(f\"\\n‚è±Ô∏è TIME ESTIMATE:\")\n    print(f\"   ‚Ä¢ Total training: ~60-80 minutes\")\n    print(f\"   ‚Ä¢ Epochs: 50\")\n    print(f\"   ‚Ä¢ Warmup epochs: 5\")\n    \n    print(f\"\\nüîç ATTENTION VISUALIZATION:\")\n    print(f\"   ‚Ä¢ Attention maps will be saved during training\")\n    print(f\"   ‚Ä¢ Shows what parts of image the model focuses on\")\n    \n    # Run experiment\n    result = manager.run_vit_experiment(vit_model)\n    \n    # Display results\n    print(f\"\\n{'='*80}\")\n    print(\"VISION TRANSFORMER RESULTS SUMMARY\")\n    print(f\"{'='*80}\")\n    \n    print(f\"\\nüèÜ FINAL PERFORMANCE:\")\n    print(f\"   Test Accuracy: {result['test_accuracy']:.4f}\")\n    print(f\"   Test F1 Score: {result['test_f1']:.4f}\")\n    print(f\"   Best Validation F1: {result['best_val_f1']:.4f}\")\n    \n    print(f\"\\nüìà TRAINING INSIGHTS:\")\n    print(f\"   Train-Val Accuracy Gap: {result['train_val_gap']:.4f}\")\n    if result['train_val_gap'] > 0.15:\n        print(f\"   ‚ö†Ô∏è  Significant overfitting detected\")\n    elif result['train_val_gap'] < 0.05:\n        print(f\"   ‚úÖ Excellent generalization\")\n    \n    print(f\"\\nüîç TRANSFORMER-SPECIFIC ANALYSIS:\")\n    print(f\"   ‚Ä¢ Attention maps saved to /kaggle/working/\")\n    print(f\"   ‚Ä¢ Model checkpoints saved\")\n    print(f\"   ‚Ä¢ Training history plots generated\")\n    \n    manager.compare_results()\n    \n    print(f\"\\n‚úÖ VISION TRANSFORMER ANALYSIS COMPLETE!\")\n    print(f\"üìÅ Output files:\")\n    print(f\"   - vit_results.csv (performance metrics)\")\n    print(f\"   - vit_results_summary_*.png (visual summary)\")\n    print(f\"   - vit_training_history_*.png (training curves)\")\n    print(f\"   - attention_*.png (attention visualization)\")\n    print(f\"   - confusion_matrix_*.png\")\n    print(f\"   - best_*.pth (trained model)\")\n    \n    return manager\n\ndef quick_vit_test():\n    \"\"\"\n    Quick test with fewer epochs\n    \"\"\"\n    print(\"\\n‚ö° QUICK ViT TEST (30 epochs)\")\n    print(\"=\" * 60)\n    \n    manager = ViTExperimentManager(\n        data_dir=\"/kaggle/input/processed-images-224x224\",\n        resolution='224x224',\n        batch_size=16\n    )\n    \n    # Prepare data\n    train_loader, val_loader, test_loader = manager.prepare_dataloaders()\n    \n    # Create model\n    model = VisionTransformer.create_model(\n        model_name='vit_base_patch16_224',\n        num_classes=4,\n        pretrained=True,\n        img_size=224\n    )\n    \n    # Quick trainer with fewer epochs\n    class QuickViTTrainer(ViTTrainer):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.warmup_epochs = 3\n            self.total_epochs = 30\n    \n    trainer = QuickViTTrainer(\n        model, device, train_loader, val_loader, test_loader,\n        model_name='vit_base_patch16_224'\n    )\n    \n    history = trainer.train(num_epochs=30, save_path='/kaggle/working/best_vit_quick.pth')\n    \n    print(f\"\\n‚úÖ QUICK TEST COMPLETE!\")\n    print(f\"   Model: ViT-B/16\")\n    print(f\"   Test F1: {history['test_f1']:.4f}\")\n    print(f\"   Test Accuracy: {history['test_acc']:.4f}\")\n    \n    return manager\n\n# ==================== EXECUTION ====================\n\nif __name__ == \"__main__\":\n    \"\"\"\n    EXECUTE VISION TRANSFORMER ANALYSIS\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üéØ VISION TRANSFORMER (ViT-B/16) FOR CLOVE GRADING\")\n    print(\"=\"*80)\n    \n    print(\"\\nüîß EXECUTION OPTIONS:\")\n    print(\"1. Full ViT analysis (50 epochs) - ~60-80 minutes\")\n    print(\"2. Quick ViT test (30 epochs) - ~35-45 minutes\")\n    \n    # Uncomment your preferred option:\n    \n    # Option 1: Full analysis (recommended)\n    manager = run_vit_analysis()\n    \n    # Option 2: Quick test\n    # manager = quick_vit_test()\n\nprint(\"\\n‚úÖ VISION TRANSFORMER CODE READY!\")\nprint(\"üöÄ Run the cell to start ViT-B/16 training with attention visualization\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}
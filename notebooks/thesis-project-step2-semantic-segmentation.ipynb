{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13099300,"sourceType":"datasetVersion","datasetId":8297594}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================== KAGGLE ENVIRONMENT SETUP ====================\n# Run this cell first in your Kaggle notebook\n\n# Install required packages\n!pip install -q seaborn  # For advanced plotting (might need update)\n!pip install -q tqdm     # For progress bars (usually pre-installed)\n\n# Check if key packages are available (most should be pre-installed in Kaggle)\nimport sys\n\n# Check Python version\nprint(f\"Python version: {sys.version}\")\n\n# Test imports and show versions\ntry:\n    import torch\n    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n    print(f\"   CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"   GPU count: {torch.cuda.device_count()}\")\n        for i in range(torch.cuda.device_count()):\n            print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\nexcept ImportError:\n    print(\"‚ùå PyTorch not available\")\n\ntry:\n    import torchvision\n    print(f\"‚úÖ TorchVision: {torchvision.__version__}\")\nexcept ImportError:\n    print(\"‚ùå TorchVision not available\")\n\ntry:\n    import cv2\n    print(f\"‚úÖ OpenCV: {cv2.__version__}\")\nexcept ImportError:\n    print(\"‚ùå OpenCV not available - installing...\")\n    !pip install opencv-python\n    import cv2\n    print(f\"‚úÖ OpenCV installed: {cv2.__version__}\")\n\ntry:\n    import numpy as np\n    print(f\"‚úÖ NumPy: {np.__version__}\")\nexcept ImportError:\n    print(\"‚ùå NumPy not available\")\n\ntry:\n    import pandas as pd\n    print(f\"‚úÖ Pandas: {pd.__version__}\")\nexcept ImportError:\n    print(\"‚ùå Pandas not available\")\n\ntry:\n    from PIL import Image\n    print(f\"‚úÖ Pillow (PIL): Available\")\nexcept ImportError:\n    print(\"‚ùå Pillow not available\")\n\ntry:\n    import matplotlib.pyplot as plt\n    import matplotlib\n    print(f\"‚úÖ Matplotlib: {matplotlib.__version__}\")\nexcept ImportError:\n    print(\"‚ùå Matplotlib not available\")\n\ntry:\n    import seaborn as sns\n    print(f\"‚úÖ Seaborn: {sns.__version__}\")\nexcept ImportError:\n    print(\"‚ùå Seaborn not available - should be installed above\")\n\ntry:\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    import sklearn\n    print(f\"‚úÖ Scikit-learn: {sklearn.__version__}\")\nexcept ImportError:\n    print(\"‚ùå Scikit-learn not available\")\n\ntry:\n    from tqdm import tqdm\n    print(f\"‚úÖ TQDM: Available\")\nexcept ImportError:\n    print(\"‚ùå TQDM not available - should be installed above\")\n\n# Test basic functionality\nprint(\"\\nüîç Testing Basic Functionality:\")\n\n# Test file system access\nimport os\nif os.path.exists(\"/kaggle/input\"):\n    print(\"‚úÖ Kaggle input directory accessible\")\n    if os.path.exists(\"/kaggle/input/znz-cloves\"):\n        print(\"‚úÖ Dataset found at /kaggle/input/znz-cloves\")\n        \n        # List dataset contents\n        print(\"\\nüìÅ Dataset Structure:\")\n        for item in os.listdir(\"/kaggle/input/znz-cloves\"):\n            item_path = os.path.join(\"/kaggle/input/znz-cloves\", item)\n            if os.path.isdir(item_path):\n                count = len([f for f in os.listdir(item_path) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))])\n                print(f\"   üìÇ {item}: {count} images\")\n            else:\n                print(f\"   üìÑ {item}\")\n    else:\n        print(\"‚ùå Dataset not found - make sure it's uploaded as 'znz-cloves'\")\nelse:\n    print(\"‚ùå Kaggle input directory not found\")\n\nif os.path.exists(\"/kaggle/working\"):\n    print(\"‚úÖ Kaggle working directory accessible\")\nelse:\n    print(\"‚ùå Kaggle working directory not found\")\n\n# Test GPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    # Test GPU memory\n    try:\n        test_tensor = torch.randn(100, 100).to(device)\n        print(f\"‚úÖ GPU memory test passed\")\n        print(f\"   Allocated: {torch.cuda.memory_allocated()/1024**2:.1f} MB\")\n        print(f\"   Cached: {torch.cuda.memory_reserved()/1024**2:.1f} MB\")\n        del test_tensor\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"‚ùå GPU test failed: {e}\")\nelse:\n    print(\"‚ùå CUDA not available\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üöÄ ENVIRONMENT SETUP COMPLETE!\")\nprint(\"=\"*60)\n\n# Set random seeds for reproducibility\nimport random\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\nprint(\"üéØ Random seeds set for reproducible results\")\nprint(\"üìù You can now run the main pipeline code!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw, ImageFont\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport zipfile\nimport shutil\nimport json\nfrom collections import defaultdict, Counter\nimport random\nfrom datetime import datetime\n\n# Set up Kaggle environment\nprint(\"üöÄ Context-Aware Pipeline for Grade Classification\")\nprint(\"Environment: Kaggle with T4 x2 GPU\")\nprint(\"=\" * 60)\n\n# Configure device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üî• Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU Count: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n\n# ==================== STEP 1: DATASET EXPLORATION AND ANALYSIS ====================\n\nclass DatasetExplorer:\n    def __init__(self, base_path=\"/kaggle/input/znz-cloves\"):\n        self.base_path = base_path\n        self.grade_paths = {\n            'Grade 1': f\"{base_path}/Grade 1\",\n            'Grade 2': f\"{base_path}/Grade 2\", \n            'Grade 3': f\"{base_path}/Grade 3\",\n            'Grade 4': f\"{base_path}/Grade 4\"\n        }\n        self.group_paths = {\n            'Group Grade 1': f\"{base_path}/group_photos/grade1\",\n            'Group Grade 2': f\"{base_path}/group_photos/grade2\",\n            'Group Grade 3': f\"{base_path}/group_photos/grade3\",\n            'Group Grade 4': f\"{base_path}/group_photos/grade4\"\n        }\n        self.all_paths = {**self.grade_paths, **self.group_paths}\n        self.dataset_stats = {}\n    \n    def explore_dataset(self):\n        \"\"\"Comprehensive dataset exploration with visualizations and statistics\"\"\"\n        print(\"üìä STEP 1: Dataset Exploration and Analysis\")\n        print(\"=\" * 50)\n        \n        # Analyze dataset structure\n        self._analyze_structure()\n        \n        # Create sample visualization\n        self._create_sample_grid()\n        \n        # Analyze image properties\n        self._analyze_image_properties()\n        \n        # Save exploration results\n        self._save_exploration_report()\n        \n        return self.dataset_stats\n    \n    def _analyze_structure(self):\n        \"\"\"Analyze dataset structure and count files\"\"\"\n        print(\"üîç Analyzing dataset structure...\")\n        \n        total_images = 0\n        for category, path in self.all_paths.items():\n            if os.path.exists(path):\n                image_files = [f for f in os.listdir(path) \n                             if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n                count = len(image_files)\n                total_images += count\n                self.dataset_stats[category] = {\n                    'count': count,\n                    'path': path,\n                    'files': image_files[:10]  # Store first 10 filenames as samples\n                }\n                print(f\"   {category}: {count} images\")\n            else:\n                print(f\"   ‚ö†Ô∏è {category}: Path not found - {path}\")\n                self.dataset_stats[category] = {'count': 0, 'path': path, 'files': []}\n        \n        print(f\"\\nüìà Total Images: {total_images}\")\n        self.dataset_stats['total_images'] = total_images\n    \n    def _create_sample_grid(self):\n        \"\"\"Create a grid showing sample images from each grade\"\"\"\n        print(\"üñºÔ∏è Creating sample image grid...\")\n        \n        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n        fig.suptitle('Sample Images from Each Grade Category', fontsize=16, fontweight='bold')\n        \n        categories = list(self.all_paths.keys())\n        sample_info = []\n        \n        for idx, category in enumerate(categories):\n            row = idx // 4\n            col = idx % 4\n            \n            path = self.all_paths[category]\n            if os.path.exists(path):\n                image_files = [f for f in os.listdir(path) \n                             if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n                \n                if image_files:\n                    # Select random sample\n                    sample_file = random.choice(image_files)\n                    sample_path = os.path.join(path, sample_file)\n                    \n                    try:\n                        # Load and display image\n                        img = cv2.imread(sample_path)\n                        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                        \n                        axes[row, col].imshow(img_rgb)\n                        axes[row, col].set_title(f\"{category}\\n({len(image_files)} images)\", \n                                               fontsize=10, fontweight='bold')\n                        axes[row, col].axis('off')\n                        \n                        # Store sample info\n                        h, w = img.shape[:2]\n                        sample_info.append({\n                            'category': category,\n                            'filename': sample_file,\n                            'dimensions': f\"{w}x{h}\",\n                            'file_size': os.path.getsize(sample_path) / 1024  # KB\n                        })\n                        \n                    except Exception as e:\n                        axes[row, col].text(0.5, 0.5, f\"Error loading\\n{category}\", \n                                          ha='center', va='center')\n                        axes[row, col].set_title(category)\n                        axes[row, col].axis('off')\n                else:\n                    axes[row, col].text(0.5, 0.5, f\"No images\\nin {category}\", \n                                      ha='center', va='center')\n                    axes[row, col].set_title(category)\n                    axes[row, col].axis('off')\n            else:\n                axes[row, col].text(0.5, 0.5, f\"Path not found\\n{category}\", \n                                  ha='center', va='center')\n                axes[row, col].set_title(category)\n                axes[row, col].axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('dataset_sample_grid.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        # Save sample info\n        self.dataset_stats['sample_info'] = sample_info\n        print(\"‚úÖ Sample grid saved as 'dataset_sample_grid.png'\")\n    \n    def _analyze_image_properties(self):\n        \"\"\"Analyze image properties across the dataset\"\"\"\n        print(\"üìê Analyzing image properties...\")\n        \n        all_properties = []\n        \n        for category, path in self.all_paths.items():\n            if os.path.exists(path) and self.dataset_stats[category]['count'] > 0:\n                image_files = [f for f in os.listdir(path) \n                             if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n                \n                # Sample up to 50 images per category for analysis\n                sample_files = random.sample(image_files, min(50, len(image_files)))\n                \n                for file_name in tqdm(sample_files, desc=f\"Analyzing {category}\"):\n                    try:\n                        img_path = os.path.join(path, file_name)\n                        img = cv2.imread(img_path)\n                        \n                        if img is not None:\n                            h, w, c = img.shape\n                            file_size = os.path.getsize(img_path) / 1024  # KB\n                            \n                            # Calculate basic statistics\n                            mean_brightness = np.mean(img)\n                            std_brightness = np.std(img)\n                            \n                            all_properties.append({\n                                'category': category,\n                                'filename': file_name,\n                                'width': w,\n                                'height': h,\n                                'channels': c,\n                                'aspect_ratio': w / h,\n                                'total_pixels': w * h,\n                                'file_size_kb': file_size,\n                                'mean_brightness': mean_brightness,\n                                'std_brightness': std_brightness\n                            })\n                    except Exception as e:\n                        print(f\"   ‚ö†Ô∏è Error analyzing {file_name}: {e}\")\n        \n        # Convert to DataFrame for analysis\n        df = pd.DataFrame(all_properties)\n        \n        # Generate summary statistics\n        if len(df) > 0:\n            summary_stats = {\n                'total_analyzed': len(df),\n                'avg_width': df['width'].mean(),\n                'avg_height': df['height'].mean(),\n                'avg_aspect_ratio': df['aspect_ratio'].mean(),\n                'avg_file_size_kb': df['file_size_kb'].mean(),\n                'width_range': (df['width'].min(), df['width'].max()),\n                'height_range': (df['height'].min(), df['height'].max()),\n                'aspect_ratio_range': (df['aspect_ratio'].min(), df['aspect_ratio'].max()),\n                'common_dimensions': {f\"{dims[0]}x{dims[1]}\": count for dims, count in df.groupby(['width', 'height']).size().nlargest(5).items()}\n            }\n            \n            self.dataset_stats['image_properties'] = summary_stats\n            \n            # Create property visualizations\n            self._create_property_plots(df)\n            \n            print(f\"üìä Analyzed {len(df)} images\")\n            print(f\"   Average dimensions: {summary_stats['avg_width']:.0f}x{summary_stats['avg_height']:.0f}\")\n            print(f\"   Average aspect ratio: {summary_stats['avg_aspect_ratio']:.2f}\")\n            print(f\"   Average file size: {summary_stats['avg_file_size_kb']:.1f} KB\")\n        else:\n            print(\"‚ö†Ô∏è No images found for property analysis\")\n    \n    def _create_property_plots(self, df):\n        \"\"\"Create visualizations for image properties\"\"\"\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        fig.suptitle('Dataset Image Properties Analysis', fontsize=16, fontweight='bold')\n        \n        # 1. Dimension distribution\n        axes[0, 0].scatter(df['width'], df['height'], alpha=0.6, c=df['category'].astype('category').cat.codes)\n        axes[0, 0].set_xlabel('Width (pixels)')\n        axes[0, 0].set_ylabel('Height (pixels)')\n        axes[0, 0].set_title('Image Dimensions Distribution')\n        \n        # 2. Aspect ratio distribution\n        axes[0, 1].hist(df['aspect_ratio'], bins=30, alpha=0.7, edgecolor='black')\n        axes[0, 1].set_xlabel('Aspect Ratio (W/H)')\n        axes[0, 1].set_ylabel('Frequency')\n        axes[0, 1].set_title('Aspect Ratio Distribution')\n        axes[0, 1].axvline(1.0, color='red', linestyle='--', label='Square (1:1)')\n        axes[0, 1].legend()\n        \n        # 3. File size distribution\n        axes[0, 2].hist(df['file_size_kb'], bins=30, alpha=0.7, edgecolor='black')\n        axes[0, 2].set_xlabel('File Size (KB)')\n        axes[0, 2].set_ylabel('Frequency')\n        axes[0, 2].set_title('File Size Distribution')\n        \n        # 4. Brightness distribution by category\n        for category in df['category'].unique():\n            subset = df[df['category'] == category]\n            axes[1, 0].hist(subset['mean_brightness'], alpha=0.6, label=category, bins=20)\n        axes[1, 0].set_xlabel('Mean Brightness')\n        axes[1, 0].set_ylabel('Frequency')\n        axes[1, 0].set_title('Brightness Distribution by Category')\n        axes[1, 0].legend()\n        \n        # 5. Image count per category\n        category_counts = df['category'].value_counts()\n        axes[1, 1].bar(range(len(category_counts)), category_counts.values)\n        axes[1, 1].set_xticks(range(len(category_counts)))\n        axes[1, 1].set_xticklabels(category_counts.index, rotation=45, ha='right')\n        axes[1, 1].set_ylabel('Number of Images Analyzed')\n        axes[1, 1].set_title('Analyzed Images per Category')\n        \n        # 6. Aspect ratio by category\n        df.boxplot(column='aspect_ratio', by='category', ax=axes[1, 2])\n        axes[1, 2].set_title('Aspect Ratio by Category')\n        axes[1, 2].set_xlabel('Category')\n        axes[1, 2].set_ylabel('Aspect Ratio')\n        \n        plt.tight_layout()\n        plt.savefig('dataset_properties_analysis.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        print(\"‚úÖ Property analysis plots saved as 'dataset_properties_analysis.png'\")\n    \n    def _save_exploration_report(self):\n        \"\"\"Save comprehensive exploration report\"\"\"\n        report = {\n            'exploration_date': datetime.now().isoformat(),\n            'dataset_structure': self.dataset_stats,\n            'summary': {\n                'total_categories': len(self.all_paths),\n                'categories_with_data': sum(1 for stats in self.dataset_stats.values() \n                                          if isinstance(stats, dict) and stats.get('count', 0) > 0),\n                'total_images': self.dataset_stats.get('total_images', 0)\n            }\n        }\n        \n        # Save as JSON\n        with open('dataset_exploration_report.json', 'w') as f:\n            json.dump(report, f, indent=2, default=str)\n        \n        # Save as readable text\n        with open('dataset_exploration_report.txt', 'w') as f:\n            f.write(\"DATASET EXPLORATION REPORT\\n\")\n            f.write(\"=\" * 50 + \"\\n\\n\")\n            f.write(f\"Exploration Date: {report['exploration_date']}\\n\\n\")\n            \n            f.write(\"DATASET STRUCTURE:\\n\")\n            f.write(\"-\" * 20 + \"\\n\")\n            for category, stats in self.dataset_stats.items():\n                if isinstance(stats, dict) and 'count' in stats:\n                    f.write(f\"{category}: {stats['count']} images\\n\")\n            \n            f.write(f\"\\nTOTAL IMAGES: {self.dataset_stats.get('total_images', 0)}\\n\\n\")\n            \n            if 'image_properties' in self.dataset_stats:\n                props = self.dataset_stats['image_properties']\n                f.write(\"IMAGE PROPERTIES SUMMARY:\\n\")\n                f.write(\"-\" * 25 + \"\\n\")\n                f.write(f\"Images Analyzed: {props['total_analyzed']}\\n\")\n                f.write(f\"Average Dimensions: {props['avg_width']:.0f}x{props['avg_height']:.0f}\\n\")\n                f.write(f\"Average Aspect Ratio: {props['avg_aspect_ratio']:.2f}\\n\")\n                f.write(f\"Average File Size: {props['avg_file_size_kb']:.1f} KB\\n\")\n                f.write(f\"Width Range: {props['width_range'][0]}-{props['width_range'][1]}\\n\")\n                f.write(f\"Height Range: {props['height_range'][0]}-{props['height_range'][1]}\\n\")\n        \n        print(\"‚úÖ Exploration report saved:\")\n        print(\"   - dataset_exploration_report.json\")\n        print(\"   - dataset_exploration_report.txt\")\n\n# ==================== STEP 2: HIGH-QUALITY IMAGE PREPROCESSING ====================\n\ndef resize_with_padding(image, target_size, interpolation_method):\n    \"\"\"\n    Resize image while maintaining aspect ratio using padding (letterboxing).\n    This preserves the original image proportions without distortion.\n    \"\"\"\n    h, w = image.shape[:2]\n    target_w, target_h = target_size\n    \n    # Calculate scaling factor to fit image within target size\n    scale = min(target_w / w, target_h / h)\n    \n    # Calculate new dimensions\n    new_w = int(w * scale)\n    new_h = int(h * scale)\n    \n    # Resize image using specified interpolation\n    resized = cv2.resize(image, (new_w, new_h), interpolation=interpolation_method)\n    \n    # Create new image with target dimensions and fill with white background\n    padded = np.full((target_h, target_w, 3), 255, dtype=np.uint8)\n    \n    # Calculate padding offsets to center the image\n    y_offset = (target_h - new_h) // 2\n    x_offset = (target_w - new_w) // 2\n    \n    # Place resized image in center of padded canvas\n    padded[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n    \n    return padded\n\ndef process_and_zip_images(base_path=\"/kaggle/input/znz-cloves\"):\n    \"\"\"\n    Process all images with high-quality resizing and create organized zip files\n    \"\"\"\n    print(\"üì∏ STEP 2: High-Quality Image Processing\")\n    print(\"=\" * 50)\n    \n    # Define all paths including group photos\n    all_folders = {\n        \"Grade 1\": f\"{base_path}/Grade 1\",\n        \"Grade 2\": f\"{base_path}/Grade 2\",\n        \"Grade 3\": f\"{base_path}/Grade 3\", \n        \"Grade 4\": f\"{base_path}/Grade 4\",\n        \"Group_Grade_1\": f\"{base_path}/group_photos/grade1\",\n        \"Group_Grade_2\": f\"{base_path}/group_photos/grade2\",\n        \"Group_Grade_3\": f\"{base_path}/group_photos/grade3\",\n        \"Group_Grade_4\": f\"{base_path}/group_photos/grade4\"\n    }\n    \n    # Create output directories\n    output_base = \"/kaggle/working/processed_images\"\n    os.makedirs(output_base, exist_ok=True)\n    \n    processing_log = []\n    \n    for folder_name, input_path in all_folders.items():\n        if not os.path.exists(input_path):\n            print(f\"‚ö†Ô∏è Folder not found: {input_path}\")\n            continue\n            \n        # Get image files\n        image_files = [f for f in os.listdir(input_path) \n                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n        \n        if not image_files:\n            print(f\"‚ö†Ô∏è No images found in: {folder_name}\")\n            continue\n        \n        print(f\"üîÑ Processing {len(image_files)} images from {folder_name}\")\n        \n        # Create output folders for different resolutions\n        output_224 = os.path.join(output_base, f\"{folder_name}_224x224\")\n        output_512 = os.path.join(output_base, f\"{folder_name}_512x512\")\n        os.makedirs(output_224, exist_ok=True)\n        os.makedirs(output_512, exist_ok=True)\n        \n        successful_224 = 0\n        successful_512 = 0\n        \n        for i, file_name in enumerate(tqdm(image_files, desc=f\"Processing {folder_name}\")):\n            try:\n                # Load image\n                img_path = os.path.join(input_path, file_name)\n                img = cv2.imread(img_path)\n                \n                if img is None:\n                    raise ValueError(\"Failed to load image\")\n                \n                h, w = img.shape[:2]\n                \n                # Determine interpolation method based on scaling\n                if max(h, w) > 224:\n                    interp_224 = cv2.INTER_AREA\n                else:\n                    interp_224 = cv2.INTER_CUBIC\n                \n                if max(h, w) > 512:\n                    interp_512 = cv2.INTER_AREA\n                else:\n                    interp_512 = cv2.INTER_CUBIC\n                \n                # Process both resolutions\n                processed_224 = resize_with_padding(img, (224, 224), interp_224)\n                processed_512 = resize_with_padding(img, (512, 512), interp_512)\n                \n                # Save processed images\n                save_path_224 = os.path.join(output_224, file_name)\n                save_path_512 = os.path.join(output_512, file_name)\n                \n                if cv2.imwrite(save_path_224, processed_224):\n                    successful_224 += 1\n                if cv2.imwrite(save_path_512, processed_512):\n                    successful_512 += 1\n                \n            except Exception as e:\n                print(f\"   ‚ùå Error processing {file_name}: {e}\")\n        \n        # Log results\n        log_entry = {\n            'folder': folder_name,\n            'total_images': len(image_files),\n            'successful_224': successful_224,\n            'successful_512': successful_512,\n            'success_rate_224': successful_224 / len(image_files),\n            'success_rate_512': successful_512 / len(image_files)\n        }\n        processing_log.append(log_entry)\n        \n        print(f\"   ‚úÖ {folder_name}: {successful_224}/{len(image_files)} (224x224), {successful_512}/{len(image_files)} (512x512)\")\n    \n    # Create zip files for download\n    print(\"\\nüì¶ Creating zip files for download...\")\n    \n    zip_files = []\n    \n    # Zip by resolution\n    for resolution in ['224x224', '512x512']:\n        zip_name = f\"processed_images_{resolution}.zip\"\n        zip_path = os.path.join(\"/kaggle/working\", zip_name)\n        \n        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for folder_name in all_folders.keys():\n                folder_path = os.path.join(output_base, f\"{folder_name}_{resolution}\")\n                if os.path.exists(folder_path):\n                    for file_name in os.listdir(folder_path):\n                        file_path = os.path.join(folder_path, file_name)\n                        arcname = f\"{folder_name}/{file_name}\"\n                        zipf.write(file_path, arcname)\n        \n        zip_files.append(zip_name)\n        print(f\"   ‚úÖ Created: {zip_name}\")\n    \n    # Save processing log\n    log_df = pd.DataFrame(processing_log)\n    log_df.to_csv('processing_log.csv', index=False)\n    \n    print(f\"\\n‚úÖ Processing complete! Created {len(zip_files)} zip files:\")\n    for zip_file in zip_files:\n        print(f\"   üìÅ {zip_file}\")\n    \n    return processing_log\n\n# ==================== STEP 3: U-NET SEGMENTATION MODEL ====================\n\nclass UNet(nn.Module):\n    \"\"\"\n    U-Net architecture for semantic segmentation\n    Based on the original U-Net paper with modern improvements\n    \"\"\"\n    def __init__(self, in_channels=3, out_channels=1, features=None):\n        super(UNet, self).__init__()\n        \n        if features is None:\n            features = [64, 128, 256, 512]\n        \n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList() \n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Down part of UNET\n        in_ch = in_channels\n        for feature in features:\n            self.downs.append(DoubleConv(in_ch, feature))\n            in_ch = feature\n        \n        # Up part of UNET\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(\n                    feature*2, feature, kernel_size=2, stride=2,\n                )\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n        \n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        skip_connections = []\n        \n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n        \n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n        \n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n            \n            if x.shape != skip_connection.shape:\n                x = nn.functional.interpolate(x, size=skip_connection.shape[2:])\n            \n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n        \n        return torch.sigmoid(self.final_conv(x))\n\nclass DoubleConv(nn.Module):\n    \"\"\"Double convolution block used in U-Net\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\n# ==================== STEP 4: CONTEXT-AWARE CLASSIFICATION MODEL ====================\n\nclass ContextAwareClassifier(nn.Module):\n    \"\"\"\n    Classification model that uses both original image and segmentation mask\n    This implements the \"Combined Input\" approach from recent research\n    \"\"\"\n    def __init__(self, num_classes=4, backbone='resnet18', pretrained=True):\n        super(ContextAwareClassifier, self).__init__()\n        \n        if backbone == 'resnet18':\n            # Modify ResNet to accept 4 channels (RGB + mask)\n            self.backbone = models.resnet18(pretrained=pretrained)\n            # Replace first conv layer to accept 4 channels\n            self.backbone.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n        elif backbone == 'vgg16':\n            self.backbone = models.vgg16(pretrained=pretrained)\n            # Modify first layer for 4 channels\n            first_layer = self.backbone.features[0]\n            self.backbone.features[0] = nn.Conv2d(4, first_layer.out_channels, \n                                                 kernel_size=first_layer.kernel_size,\n                                                 stride=first_layer.stride,\n                                                 padding=first_layer.padding)\n            self.backbone.classifier[6] = nn.Linear(4096, num_classes)\n        \n        # Initialize new weights for the additional channel\n        self._init_new_channel_weights()\n    \n    def _init_new_channel_weights(self):\n        \"\"\"Initialize weights for the new mask channel\"\"\"\n        if hasattr(self.backbone, 'conv1'):\n            # For ResNet\n            with torch.no_grad():\n                # Copy RGB weights and average for mask channel\n                old_weights = self.backbone.conv1.weight[:, :3, :, :].clone()\n                self.backbone.conv1.weight[:, :3, :, :] = old_weights\n                self.backbone.conv1.weight[:, 3:4, :, :] = old_weights.mean(dim=1, keepdim=True)\n        else:\n            # For VGG\n            with torch.no_grad():\n                old_weights = self.backbone.features[0].weight[:, :3, :, :].clone()\n                self.backbone.features[0].weight[:, :3, :, :] = old_weights\n                self.backbone.features[0].weight[:, 3:4, :, :] = old_weights.mean(dim=1, keepdim=True)\n    \n    def forward(self, x):\n        return self.backbone(x)\n\n# ==================== MAIN EXECUTION ====================\n\ndef run_context_aware_pipeline():\n    \"\"\"Execute the complete context-aware pipeline\"\"\"\n    print(\"üöÄ Starting Context-Aware Pipeline with Semantic Segmentation\")\n    print(\"=\" * 70)\n    \n    # Step 1: Dataset Exploration\n    explorer = DatasetExplorer()\n    dataset_stats = explorer.explore_dataset()\n    \n    # Step 2: High-Quality Image Processing and Zipping\n    processing_log = process_and_zip_images()\n    \n    print(\"\\nüéØ PIPELINE STATUS:\")\n    print(\"=\" * 30)\n    print(\"‚úÖ Dataset exploration complete\")\n    print(\"‚úÖ High-quality image processing complete\")\n    print(\"‚úÖ Zip files created for download\")\n    print(\"\\nüìÅ OUTPUT FILES:\")\n    print(\"- dataset_sample_grid.png\")\n    print(\"- dataset_properties_analysis.png\") \n    print(\"- dataset_exploration_report.json\")\n    print(\"- dataset_exploration_report.txt\")\n    print(\"- processed_images_224x224.zip\")\n    print(\"- processed_images_512x512.zip\")\n    print(\"- processing_log.csv\")\n    \n    print(\"\\nüîÑ NEXT STEPS:\")\n    print(\"1. Download the zip files containing processed images\")\n    print(\"2. Create manual annotations for a subset of images (for segmentation training)\")\n    print(\"3. Train U-Net segmentation model on annotated data\")\n    print(\"4. Generate masks for full dataset\")\n    print(\"5. Train context-aware classifier with image+mask inputs\")\n    \n    print(f\"\\nüíæ All files saved to /kaggle/working/\")\n    \n    return {\n        'dataset_stats': dataset_stats,\n        'processing_log': processing_log\n    }\n\n# ==================== STEP 5: ANNOTATION HELPER TOOLS ====================\n\nclass AnnotationHelper:\n    \"\"\"Helper tools for creating segmentation annotations\"\"\"\n    \n    def __init__(self, processed_images_path=\"/kaggle/working/processed_images\"):\n        self.processed_path = processed_images_path\n        self.annotations_path = \"/kaggle/working/annotations\"\n        os.makedirs(self.annotations_path, exist_ok=True)\n    \n    def create_annotation_template(self, num_samples_per_grade=5):\n        \"\"\"Create a structured annotation template for manual labeling\"\"\"\n        print(\"üè∑Ô∏è Creating Annotation Template\")\n        print(\"=\" * 40)\n        \n        # Select representative samples from each grade\n        annotation_plan = {}\n        \n        grade_folders = [f for f in os.listdir(self.processed_path) \n                        if f.endswith('224x224') and not f.startswith('Group_')]\n        \n        for folder_name in grade_folders:\n            grade = folder_name.replace('_224x224', '')\n            folder_path = os.path.join(self.processed_path, folder_name)\n            \n            if os.path.exists(folder_path):\n                image_files = [f for f in os.listdir(folder_path) \n                             if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n                \n                # Select diverse samples (by file size as proxy for complexity)\n                if len(image_files) >= num_samples_per_grade:\n                    # Get file sizes and select diverse samples\n                    file_sizes = []\n                    for img_file in image_files:\n                        size = os.path.getsize(os.path.join(folder_path, img_file))\n                        file_sizes.append((img_file, size))\n                    \n                    # Sort by size and select samples across the range\n                    file_sizes.sort(key=lambda x: x[1])\n                    indices = np.linspace(0, len(file_sizes)-1, num_samples_per_grade, dtype=int)\n                    selected_files = [file_sizes[i][0] for i in indices]\n                else:\n                    selected_files = image_files[:num_samples_per_grade]\n                \n                annotation_plan[grade] = selected_files\n        \n        # Create annotation directory structure\n        for grade, files in annotation_plan.items():\n            grade_dir = os.path.join(self.annotations_path, grade)\n            images_dir = os.path.join(grade_dir, 'images')\n            masks_dir = os.path.join(grade_dir, 'masks')\n            \n            os.makedirs(images_dir, exist_ok=True)\n            os.makedirs(masks_dir, exist_ok=True)\n            \n            # Copy selected images\n            source_folder = os.path.join(self.processed_path, f\"{grade}_224x224\")\n            for file_name in files:\n                source_path = os.path.join(source_folder, file_name)\n                dest_path = os.path.join(images_dir, file_name)\n                shutil.copy2(source_path, dest_path)\n                \n                # Create empty mask template\n                mask_name = file_name.replace('.jpg', '_mask.png').replace('.jpeg', '_mask.png')\n                mask_path = os.path.join(masks_dir, mask_name)\n                \n                # Create blank mask (black image)\n                blank_mask = np.zeros((224, 224), dtype=np.uint8)\n                cv2.imwrite(mask_path, blank_mask)\n        \n        # Create annotation instructions\n        instructions = \"\"\"\nANNOTATION INSTRUCTIONS\n======================\n\n1. OBJECTIVE: Create pixel-level masks for clove objects in each image\n\n2. TOOLS RECOMMENDED:\n   - CVAT (Computer Vision Annotation Tool) - cvat.ai\n   - LabelMe - github.com/wkentaro/labelme\n   - Roboflow - roboflow.com\n   - Or any image annotation software supporting segmentation\n\n3. ANNOTATION GUIDELINES:\n   - Mark the main clove object(s) in white (255)\n   - Background should remain black (0)\n   - Be precise around object boundaries\n   - Include all visible parts of the clove\n   - For group photos, annotate individual cloves separately if possible\n\n4. FILE NAMING:\n   - Keep image names unchanged\n   - Mask files should be: [original_name]_mask.png\n   - Masks should be 224x224 grayscale images\n\n5. QUALITY CHECKS:\n   - Mask should have clean edges\n   - No gaps inside the object\n   - No stray pixels outside the object\n   - Check mask alignment with original image\n\n6. DIRECTORY STRUCTURE:\n   annotations/\n   ‚îú‚îÄ‚îÄ Grade_1/\n   ‚îÇ   ‚îú‚îÄ‚îÄ images/ (original images)\n   ‚îÇ   ‚îî‚îÄ‚îÄ masks/  (your annotations)\n   ‚îú‚îÄ‚îÄ Grade_2/\n   ‚îÇ   ‚îú‚îÄ‚îÄ images/\n   ‚îÇ   ‚îî‚îÄ‚îÄ masks/\n   ‚îî‚îÄ‚îÄ ... (etc)\n\nTotal images to annotate: {}\n\nAfter annotation, use the train_segmentation_model() function to train U-Net.\n        \"\"\".format(sum(len(files) for files in annotation_plan.values()))\n        \n        with open(os.path.join(self.annotations_path, 'ANNOTATION_INSTRUCTIONS.txt'), 'w') as f:\n            f.write(instructions)\n        \n        # Save annotation plan\n        plan_df = pd.DataFrame([(grade, len(files), files) for grade, files in annotation_plan.items()],\n                              columns=['Grade', 'Number_of_Images', 'Selected_Files'])\n        plan_df.to_csv(os.path.join(self.annotations_path, 'annotation_plan.csv'), index=False)\n        \n        print(f\"‚úÖ Annotation template created in: {self.annotations_path}\")\n        print(f\"üìä Total images selected for annotation: {sum(len(files) for files in annotation_plan.values())}\")\n        for grade, files in annotation_plan.items():\n            print(f\"   {grade}: {len(files)} images\")\n        \n        return annotation_plan\n\n# ==================== STEP 6: SEGMENTATION TRAINING ====================\n\nclass SegmentationDataset(Dataset):\n    \"\"\"Dataset for training segmentation model\"\"\"\n    \n    def __init__(self, images_dir, masks_dir, transform=None):\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n        \n        self.images = [f for f in os.listdir(images_dir) \n                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.images_dir, img_name)\n        \n        # Corresponding mask\n        mask_name = img_name.replace('.jpg', '_mask.png').replace('.jpeg', '_mask.png')\n        mask_path = os.path.join(self.masks_dir, mask_name)\n        \n        # Load image and mask\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        mask = mask.astype(np.float32) / 255.0  # Normalize to [0, 1]\n        \n        if self.transform:\n            # Apply same transform to both image and mask\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n        \n        # Convert to tensors\n        image = transforms.ToTensor()(image)\n        mask = torch.from_numpy(mask).unsqueeze(0)  # Add channel dimension\n        \n        return image, mask\n\ndef train_segmentation_model(annotations_path=\"/kaggle/working/annotations\", \n                           num_epochs=50, batch_size=4, learning_rate=1e-4):\n    \"\"\"Train U-Net segmentation model on annotated data\"\"\"\n    print(\"üß† Training U-Net Segmentation Model\")\n    print(\"=\" * 40)\n    \n    # Check if annotations exist\n    if not os.path.exists(annotations_path):\n        print(\"‚ùå Annotations not found! Please create annotations first using AnnotationHelper\")\n        return None\n    \n    # Collect all annotated data\n    all_images_dirs = []\n    all_masks_dirs = []\n    \n    for grade_folder in os.listdir(annotations_path):\n        grade_path = os.path.join(annotations_path, grade_folder)\n        if os.path.isdir(grade_path):\n            images_dir = os.path.join(grade_path, 'images')\n            masks_dir = os.path.join(grade_path, 'masks')\n            \n            if os.path.exists(images_dir) and os.path.exists(masks_dir):\n                all_images_dirs.append(images_dir)\n                all_masks_dirs.append(masks_dir)\n    \n    if not all_images_dirs:\n        print(\"‚ùå No valid annotation directories found!\")\n        return None\n    \n    # Note: For demonstration, we'll create a simple training setup\n    # In practice, you'd need proper augmentation library like albumentations\n    print(\"‚ö†Ô∏è  Note: This is a simplified training setup.\")\n    print(\"   For production, consider using albumentations for augmentation\")\n    \n    # Initialize model\n    model = UNet(in_channels=3, out_channels=1).to(device)\n    \n    # Loss and optimizer\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    print(f\"‚úÖ Model initialized on {device}\")\n    print(f\"üìä Training parameters: epochs={num_epochs}, batch_size={batch_size}, lr={learning_rate}\")\n    \n    # For this demo, we'll show the training structure\n    training_structure = {\n        'model': 'U-Net',\n        'input_size': '224x224',\n        'architecture': 'Encoder-Decoder with skip connections',\n        'output': 'Binary segmentation mask',\n        'loss_function': 'Binary Cross-Entropy',\n        'optimizer': 'Adam',\n        'data_augmentation': 'Recommended: rotation, flip, brightness, contrast'\n    }\n    \n    print(\"üèóÔ∏è Training Structure:\")\n    for key, value in training_structure.items():\n        print(f\"   {key}: {value}\")\n    \n    return model\n\n# ==================== STEP 7: CONTEXT-AWARE DATASET ====================\n\nclass ContextAwareDataset(Dataset):\n    \"\"\"Dataset that combines images with segmentation masks for context-aware classification\"\"\"\n    \n    def __init__(self, images_dir, masks_dir, labels, transform=None):\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.labels = labels\n        self.transform = transform\n        \n        self.image_files = [f for f in os.listdir(images_dir) \n                           if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.images_dir, img_name)\n        \n        # Load original image\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Load corresponding mask\n        mask_name = img_name.replace('.jpg', '_predicted_mask.png').replace('.jpeg', '_predicted_mask.png')\n        mask_path = os.path.join(self.masks_dir, mask_name)\n        \n        if os.path.exists(mask_path):\n            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n            mask = mask.astype(np.float32) / 255.0\n        else:\n            # Fallback: create dummy mask if prediction doesn't exist\n            mask = np.ones((224, 224), dtype=np.float32)\n        \n        if self.transform:\n            image = self.transform(Image.fromarray(image))\n        \n        # Add mask as 4th channel\n        mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n        combined_input = torch.cat([image, mask_tensor], dim=0)  # 4 channels: RGB + mask\n        \n        label = self.labels[idx]\n        return combined_input, label\n\n# ==================== STEP 8: COMPLETE PIPELINE EXECUTION ====================\n\ndef run_complete_context_aware_pipeline():\n    \"\"\"Run the complete context-aware pipeline with all steps\"\"\"\n    \n    results = run_context_aware_pipeline()\n    \n    # Create annotation helper\n    print(\"\\nüè∑Ô∏è STEP 3: Setting up Annotation Framework\")\n    print(\"=\" * 50)\n    \n    annotator = AnnotationHelper()\n    annotation_plan = annotator.create_annotation_template(num_samples_per_grade=10)\n    \n    # Demonstrate segmentation training setup\n    print(\"\\nüß† STEP 4: Segmentation Model Training Setup\")\n    print(\"=\" * 50)\n    \n    segmentation_model = train_segmentation_model()\n    \n    print(\"\\nüéØ CONTEXT-AWARE PIPELINE COMPLETION STATUS\")\n    print(\"=\" * 55)\n    print(\"‚úÖ Dataset exploration and visualization complete\")\n    print(\"‚úÖ High-quality image preprocessing complete\") \n    print(\"‚úÖ Zip files created for easy download\")\n    print(\"‚úÖ Annotation framework and templates created\")\n    print(\"‚úÖ U-Net segmentation model architecture ready\")\n    print(\"‚úÖ Context-aware classifier architecture ready\")\n    \n    print(\"\\nüìã MANUAL STEPS REQUIRED:\")\n    print(\"1. üñäÔ∏è  Annotate the selected sample images (see annotations/ folder)\")\n    print(\"2. üß† Train U-Net on annotated data\")\n    print(\"3. üîÆ Generate masks for full dataset using trained U-Net\")\n    print(\"4. üéØ Train context-aware classifier with image+mask inputs\")\n    print(\"5. üìä Evaluate and compare with baseline models\")\n    \n    print(\"\\nüí° RESEARCH ADVANTAGES OF THIS APPROACH:\")\n    print(\"- Semantic understanding of object boundaries\")\n    print(\"- Robust to background variations\")\n    print(\"- Preserves fine-grained details for grading\")\n    print(\"- State-of-the-art approach for object classification\")\n    print(\"- Can handle complex backgrounds and lighting\")\n    \n    print(\"\\nüìÅ ALL OUTPUT FILES:\")\n    output_files = [\n        \"dataset_sample_grid.png\",\n        \"dataset_properties_analysis.png\",\n        \"dataset_exploration_report.json\", \n        \"dataset_exploration_report.txt\",\n        \"processed_images_224x224.zip\",\n        \"processed_images_512x512.zip\",\n        \"processing_log.csv\",\n        \"annotations/ (directory with annotation templates)\",\n        \"annotations/ANNOTATION_INSTRUCTIONS.txt\",\n        \"annotations/annotation_plan.csv\"\n    ]\n    \n    for file_name in output_files:\n        print(f\"   üìÑ {file_name}\")\n    \n    print(f\"\\nüöÄ Pipeline execution complete!\")\n    print(f\"üìä Total dataset size: {results['dataset_stats'].get('total_images', 'Unknown')} images\")\n    print(f\"üéØ Ready for manual annotation and model training phases\")\n    \n    return results\n\n# ==================== DEMO AND VISUALIZATION FUNCTIONS ====================\n\ndef create_pipeline_flowchart():\n    \"\"\"Create a visual flowchart of the context-aware pipeline\"\"\"\n    \n    fig, ax = plt.subplots(figsize=(14, 10))\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 12)\n    ax.axis('off')\n    \n    # Pipeline steps\n    steps = [\n        (\"Raw Dataset\\n(Individual + Group Photos)\", 5, 11, 'lightblue'),\n        (\"Dataset Exploration\\n& Visualization\", 5, 9.5, 'lightgreen'), \n        (\"High-Quality Preprocessing\\n(Aspect Ratio + Padding)\", 5, 8, 'lightyellow'),\n        (\"Manual Annotation\\n(Subset of Images)\", 2.5, 6.5, 'lightcoral'),\n        (\"U-Net Training\\n(Segmentation)\", 2.5, 5, 'lightpink'),\n        (\"Mask Generation\\n(Full Dataset)\", 2.5, 3.5, 'lightgray'),\n        (\"Baseline CNN\\n(Original Images)\", 7.5, 6.5, 'lightsteelblue'),\n        (\"Context-Aware CNN\\n(Images + Masks)\", 5, 2, 'gold'),\n        (\"Model Comparison\\n& Evaluation\", 5, 0.5, 'lightseagreen')\n    ]\n    \n    # Draw boxes and text\n    for text, x, y, color in steps:\n        bbox = dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.7)\n        ax.text(x, y, text, ha='center', va='center', fontsize=10, \n               fontweight='bold', bbox=bbox)\n    \n    # Draw arrows\n    arrows = [\n        (5, 10.7, 5, 9.8),  # Dataset -> Exploration\n        (5, 9.2, 5, 8.3),   # Exploration -> Preprocessing\n        (4.5, 7.7, 3, 6.8),  # Preprocessing -> Annotation\n        (2.5, 6.2, 2.5, 5.3), # Annotation -> U-Net\n        (2.5, 4.7, 2.5, 3.8), # U-Net -> Mask Generation\n        (3, 3.5, 4.5, 2.3),  # Masks -> Context-Aware\n        (5.5, 7.7, 7, 6.8),  # Preprocessing -> Baseline\n        (6.5, 6.2, 5.5, 2.3), # Baseline -> Context-Aware\n        (5, 1.7, 5, 0.8)     # Context-Aware -> Evaluation\n    ]\n    \n    for x1, y1, x2, y2 in arrows:\n        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n                   arrowprops=dict(arrowstyle='->', lw=2, color='darkblue'))\n    \n    plt.title('Context-Aware Pipeline Flowchart', fontsize=16, fontweight='bold', pad=20)\n    plt.tight_layout()\n    plt.savefig('context_aware_pipeline_flowchart.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"‚úÖ Pipeline flowchart saved as 'context_aware_pipeline_flowchart.png'\")\n\n# Run the complete pipeline\nif __name__ == \"__main__\":\n    # Create pipeline visualization first\n    create_pipeline_flowchart()\n    \n    # Run the complete pipeline\n    results = run_complete_context_aware_pipeline()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}